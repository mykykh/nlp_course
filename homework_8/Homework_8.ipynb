{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37eb6a53-85db-4835-8e00-ed288ae24583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mykyta/dev/study/nlp_course/homework/.poetry/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import evaluate\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import onnxruntime as ort\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "from optimum.onnxruntime import ORTModelForTokenClassification, ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DistilBertConfig,\n",
    "    DistilBertForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel, LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f522a183-fa07-44db-8b54-f7fe116cfaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/location_detection/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777dcb04-75b2-49eb-bdb3-df80986193fe",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fc4faea-29f0-4fe2-addb-0408bb72a102",
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_train_processed_dataset = load_dataset(\n",
    "    'parquet',\n",
    "    data_files=DATA_DIR + 'uk_geo_dataset_processed_train_av.parquet',\n",
    "    split='train[:10%]'\n",
    ")\n",
    "uk_holdout_processed_dataset = load_dataset(\n",
    "    'parquet',\n",
    "    data_files=DATA_DIR + 'uk_geo_dataset_processed_holdout_av.parquet',\n",
    "    split='train'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa815ebd-2716-46b9-ad40-52aa53b02aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv(DATA_DIR + 'competition/test.csv', converters={'locations': eval})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16857a97-2865-4f63-b1c1-a5328119be6a",
   "metadata": {},
   "source": [
    "## Roberta distilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfa4367-796b-407c-b4f7-c52a048c92c2",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1211d5d-40a8-4751-99e1-06b393c66a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model_name = 'xlm-roberta-base'\n",
    "uk_checkpoint = DATA_DIR + 'models/uk-loc/checkpoint-2000'\n",
    "\n",
    "labels = ['S', 'O', 'B-LOC', 'I-LOC']\n",
    "label2id = {l: i for i, l in enumerate(labels)}\n",
    "id2label = {i: l for i, l in enumerate(labels)}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_model = AutoModelForTokenClassification.from_pretrained(base_model_name, label2id=label2id, id2label=id2label)\n",
    "\n",
    "uk_model = PeftModel.from_pretrained(base_model, uk_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c7c453b-4963-478a-a48b-9890e43364a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distill_config():\n",
    "    return DistilBertConfig(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        label2id=label2id,\n",
    "        id2label=id2label,\n",
    "        n_layers=3,\n",
    "        n_heads=6,\n",
    "        hidden_dim=1024,\n",
    "        dim=384\n",
    "    )\n",
    "\n",
    "def get_distill_base_model():\n",
    "    return DistilBertForTokenClassification(get_distill_config())\n",
    "\n",
    "distill_uk_model = get_distill_base_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1dbab8-d889-41e3-b933-a524c7abf36c",
   "metadata": {},
   "source": [
    "### Align labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7359344-bd59-4dc7-a582-b966169c8f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_word_ids(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            # special tokens\n",
    "            current_word = word_id\n",
    "            new_labels.append(label2id[\"S\"])\n",
    "        elif word_id != current_word:\n",
    "            # start of new word\n",
    "            current_word = word_id\n",
    "            new_labels.append(label2id[labels[word_id]])\n",
    "        else:\n",
    "            # part of a word\n",
    "            label = labels[word_id]\n",
    "\n",
    "            if label == \"B-LOC\":\n",
    "                label = \"I-LOC\"\n",
    "\n",
    "            new_labels.append(label2id[label])\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def align_labels(examples):\n",
    "    bert_tokens = tokenizer(examples['tokens'], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(examples['labels']):\n",
    "        word_ids = bert_tokens.word_ids(i)\n",
    "        new_labels.append(align_labels_with_word_ids(labels, word_ids))\n",
    "\n",
    "    bert_tokens['labels'] = new_labels\n",
    "    return bert_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84da2d4d-5a90-43ca-9fa8-e40a59bc0080",
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_train_dataset = uk_train_processed_dataset.map(\n",
    "    align_labels,\n",
    "    batched=True\n",
    ")\n",
    "uk_eval_dataset = uk_holdout_processed_dataset.map(\n",
    "    align_labels,\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5115ab9d-ea36-4109-a7d5-a90b9450e8f3",
   "metadata": {},
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92035f4c-f7c0-4c5b-a0c6-ca5a85e20189",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "  logits, labels = eval_preds\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "  true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "  prediction_label = [[id2label[p] for p, l in zip(prediction, label) if l != -100]\n",
    "                      for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "  all_metrics = metric.compute(predictions=prediction_label, references=true_labels)\n",
    "  return {\n",
    "      'precision': all_metrics['overall_precision'],\n",
    "      'recall': all_metrics['overall_recall'],\n",
    "      'f1': all_metrics['overall_f1']\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e446c731-4041-4491-bbd9-51f87d3c77ac",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84775bdd-24bd-4dd6-abcb-e744175cad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillTrainer(Trainer):\n",
    "    def __init__(self, student_model, teacher_model, temperature, lambda_param, *args, **kwargs):\n",
    "        super().__init__(model=student_model, *args, **kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        self.student = student_model\n",
    "        self.loss_fn = nn.KLDivLoss()\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.teacher.to(device)\n",
    "        self.teacher.eval()\n",
    "        self.temperature = temperature\n",
    "        self.lambda_param = lambda_param\n",
    "\n",
    "    def compute_loss(self, student, inputs, return_outputs=False):\n",
    "        student_output = self.student(**inputs)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_output = self.teacher(**inputs)\n",
    "\n",
    "        soft_teacher = F.softmax(teacher_output.logits / self.temperature, dim=-1)\n",
    "        soft_student = F.log_softmax(student_output.logits / self.temperature, dim=-1)\n",
    "\n",
    "        # calculate loss for diference between student and teacher\n",
    "        distill_loss = self.loss_fn(soft_student, soft_teacher) * (self.temperature**2)\n",
    "\n",
    "        # loss for student target predictions\n",
    "        student_target_loss = student_output.loss\n",
    "\n",
    "        # combine student teacher loss and student target loss\n",
    "        loss = (1-self.lambda_param)*student_target_loss + self.lambda_param*distill_loss\n",
    "        \n",
    "        return (loss, student_output) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459691ba-7ca4-4fa3-9cc2-7624008bdecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(output_dir, student_model, teacher_model, train_dataset, eval_dataset):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        evaluation_strategy='steps',\n",
    "        save_strategy='steps',\n",
    "        logging_strategy='steps',\n",
    "        eval_steps=100,\n",
    "        logging_steps=100,\n",
    "        save_steps=100,\n",
    "        save_total_limit=3,\n",
    "        num_train_epochs=1,\n",
    "    )\n",
    "    trainer = DistillTrainer(\n",
    "        student_model=student_model,\n",
    "        teacher_model=teacher_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer),\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        temperature=2,\n",
    "        lambda_param=0.5\n",
    "    )\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0101f8a0-7b79-4827-9756-273da1469477",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    DATA_DIR + 'models/uk-distilled',\n",
    "    distill_uk_model,\n",
    "    uk_model,\n",
    "    uk_train_dataset,\n",
    "    uk_eval_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eaac81-3042-4125-944b-6877be92d202",
   "metadata": {},
   "source": [
    "### Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a12cb53-45be-4466-ba9c-24f5aca8f251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_uk_checkpoint = DATA_DIR + 'models/uk-loc/checkpoint-2000'\n",
    "distill_uk_checkpoint = DATA_DIR + 'models/uk-distilled/checkpoint-800'\n",
    "\n",
    "base_model = AutoModelForTokenClassification.from_pretrained(base_model_name, label2id=label2id, id2label=id2label)\n",
    "base_uk_model = PeftModel.from_pretrained(base_model, base_uk_checkpoint).merge_and_unload()\n",
    "base_uk_model.eval()\n",
    "\n",
    "distill_uk_model = DistilBertForTokenClassification.from_pretrained(distill_uk_checkpoint)\n",
    "distill_uk_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9d3b2af-6c62-497a-ab7c-cd6a60a07f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_model(model, dataset):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    input = tokenizer(dataset['tokens'], return_tensors='pt', max_length=50, padding='max_length', truncation=True, is_split_into_words=True)\n",
    "    labels = [p + ([0] * (50 - len(p))) for p in [t[:50] for t in dataset['labels']]]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**input).logits.cpu().detach().numpy()\n",
    "        \n",
    "    end_time = time.time()\n",
    "    return end_time - start_time, compute_metrics((logits, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba81d6cb-3ba8-4118-8f5c-f4f389e341dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model inference time: 25.507636547088623, f1: 0.4667684195920632\n",
      "Distilled model inference time: 1.730381965637207, f1: 0.42507580565545444\n",
      "Inference acceleration: 14.74, f1 degradation: 0.04169261393660878\n"
     ]
    }
   ],
   "source": [
    "base_time, base_metrics = time_model(base_uk_model, uk_eval_dataset)\n",
    "\n",
    "distill_time, distill_metrics = time_model(distill_uk_model, uk_eval_dataset)\n",
    "\n",
    "print(f\"Original model inference time: {base_time}, f1: {base_metrics['f1']}\")\n",
    "print(f\"Distilled model inference time: {distill_time}, f1: {distill_metrics['f1']}\")\n",
    "print(f\"Inference acceleration: {round(base_time/distill_time, 2)}, f1 degradation: {base_metrics['f1'] - distill_metrics['f1']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280ec45a-f16e-4be6-bea9-8f30d61bb619",
   "metadata": {},
   "source": [
    "As we can see, distilled model is 15 times faster, but metric is only ~10% worse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e7f1cc-1392-4b6b-aadc-a63646f954eb",
   "metadata": {},
   "source": [
    "## Onnx conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20a3b8b1-4898-43c9-9e49-5e653109f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_directory = DATA_DIR + 'models/onnx/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0b8272-e431-4299-ab3b-c1ddbaafa229",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_model = ORTModelForTokenClassification.from_pretrained(distill_uk_checkpoint, export=True)\n",
    "\n",
    "ort_model.save_pretrained(onnx_directory + 'regular/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05f7f633-c925-48a6-ba4b-42d6a0c4cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_uk_model = ORTModelForTokenClassification.from_pretrained(onnx_directory + 'regular/', file='model.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46d21015-da0f-451a-aba5-7e7edc35c760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Onnx model inference time: 1.6918556690216064, f1: 0.42507580565545444\n",
      "Inference acceleration: 1.02, f1 degradation: 0.0\n"
     ]
    }
   ],
   "source": [
    "onnx_time, onnx_metrics = time_model(onnx_uk_model, uk_eval_dataset)\n",
    "\n",
    "print(f\"Onnx model inference time: {onnx_time}, f1: {onnx_metrics['f1']}\")\n",
    "print(f\"Inference acceleration: {round(distill_time/onnx_time, 2)}, f1 degradation: {distill_metrics['f1'] - onnx_metrics['f1']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d686bf-2d0f-4909-abfb-254f89235486",
   "metadata": {},
   "source": [
    "Onnx model is slower than distilled model. But since metric not degraded, using onnx model for quantization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2f797c-64a5-4c2d-bca8-eda6c8192fa4",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc741d4-b8f8-42c5-9b0a-ba6dbded55df",
   "metadata": {},
   "outputs": [],
   "source": [
    "qconfig = AutoQuantizationConfig.arm64(is_static=False, per_channel=False)\n",
    "quantizer = ORTQuantizer.from_pretrained(onnx_uk_model)\n",
    "\n",
    "quantizer.quantize(save_dir=onnx_directory + 'quantized/', quantization_config=qconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "107c6f1b-63e9-425b-b6f2-a4b16be26159",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_quant_uk_model = ORTModelForTokenClassification.from_pretrained(onnx_directory + 'quantized/', file='model_quantized.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79fcc6cc-c4eb-4fb6-aa37-83ff387a6d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qunatized model inference time: 1.4115355014801025, f1: 0.41883185840707965\n",
      "Inference acceleration: 1.23, f1 degradateion: 0.006243947248374793\n"
     ]
    }
   ],
   "source": [
    "quant_time, quant_metrics = time_model(onnx_quant_uk_model, uk_eval_dataset)\n",
    "\n",
    "print(f\"Qunatized model inference time: {quant_time}, f1: {quant_metrics['f1']}\")\n",
    "print(f\"Inference acceleration: {round(distill_time/quant_time, 2)}, f1 degradateion: {distill_metrics['f1'] - quant_metrics['f1']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682ad5cf-306f-4e00-bb62-128f0c29f30f",
   "metadata": {},
   "source": [
    "As we can see, quantized model is a little bit faster than regular distilled model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6528f853-dfd5-48c8-b7d2-35c776911963",
   "metadata": {},
   "source": [
    "## FastAPI service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7061787-9049-4c67-a59c-9982d65da071",
   "metadata": {},
   "source": [
    "Code for api and docker is in ./service folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36856263-3458-4aeb-bdcf-232e6385731a",
   "metadata": {},
   "source": [
    "Predictions from distilled quantized onnx model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eedcea92-1deb-41e7-acd3-ff22c1e4e7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'texts': ['У пам’ять про загиблого випускника НаУКМА збирають кошти на меморіальну стипендію\\n\\nСтипендія, яку заснували на честь Євгена Олефіренка, покриє плату за навчання для студента чи студентки на магістерській програмі «Історія» в Могилянці.\\n\\nНа початку повномасштабного вторгнення Євген мобілізувався до 206-го київського батальйону ТрО, був командиром взводу Першої окремої бригади спеціального призначення імені Івана Богуна.\\nХлопець воював під Миколаєвом, був гранатометником і мінометником, навчав іноземних добровольців. Його підрозділ захищав Лисичанськ на Луганщині. І могилянець зумів вивести свій взвод з напівоточеного міста неушкодженим. Євген загинув під Бахмутом 7 липня 2022 року.\\n\\nНа кафедрі історії зазначають, що наразі вдалось зібрати повну суму на 2-й рік навчання, а для повної стипендії на 2023-2024 роки бракує лише 13 тис. грн. Необхідна сума: 40\\u202f000 грн. Здійснити переказ можна за посиланням. \\n\\n🔷\\xa0Підписатися на Telegram\\xa0| Instagram\\xa0| Facebook\\xa0| TikTok',\n",
       "  '🥷🏻Столичні копи врятували «Бетмена», який намагався переплисти Дніпро \\n\\nЧоловік вирішив дістатися до іншого берега та ледь не втопився. На щастя, потерпілого помітив очевидець та звернувся по допомогу до співробітників річкової поліції.\\xa0\\n\\xa0\\nІнцидент трапився вчора ввечері. \\nДіставши «Бетмена» з води на катер, правоохоронці встановили його особу. Це - іноземець, 1995 року народження, який хотів переплисти річку.',\n",
       "  'У Києві у шістьох співробітників закладу харчування виявили гостру кишкову інфекцію❗️\\n\\nВони їли страви, які самі ж готували😳\\n\\nПодробиці ⬅️',\n",
       "  '🤩 У столиці 6 вересня відкриють першу в Україні бібліотеку настільних ігор\\n\\nСпочатку планується проводити ігрові зустрічі. Також усі охочі можуть брати настілки у читальній залі, а згодом ігри зможуть давати додому, як книги.\\n\\nІгровий фонд Бібліотеки налічує понад 125 ігор. Щоб скористатися ігровим фондом, потрібно стати читачем Центральної бібліотеки ім. Т. Г. Шевченка для дітей (просп. Берестейський, 25-А).\\n\\nКиїв — пряма трансляція 👈\\nНадіслати новину',\n",
       "  '⚡️Сьогодні день народження у КРЕЩАТИК 36 🎉\\n\\nНам сьогодні, 31.08.2023, виповнюється рівно 3 роки🍾🎊\\n\\nКожен день наша команда робить так, щоб вам було цікаво і корисно прожити цей день, розказує хто насправді керує Києвом, хто і як заробляє на міському бюджеті, даруємо аналітику комісій, сесій, розповідаємо про земельний дерибан, про смотрящих, еліту і бюджет, розкриваємо схеми і таємне життя посадовців в імʼя правди і справедливості.\\n\\nМи любимо наш Київ, знаємо кожен закуток на Хрещатику 36 та говоримо те, про що всі мовчать.\\n\\nШановні кияни, прихильники, будемо вдячні в честь подарунку до нашого дня народження зробити репост, чи написати про нас відзив-привітання у своєму пості в Фейсбук, Інстаграм, Тік Ток, або Телеграм - поставивши тег #телеграмканалкрещатик36 та посилання https://t.me/khreschatyk36. Потім кидайте посилання на свій пост, або його скріншот нам на пошту khreschatyk36@protonmail.com і ми опублікуємо Ваші добрі чи не дуже слова, а також через надійного та кмітливого посередника передамо символічний презент🥳\\n\\nP.S. Запропоновані листівки можете використовувати для своїх постів, чи робіть свої, бажано смішніші😅\\n\\nІ цей, можете задонатити для ЗСУ на того, кому довіряєте🇺🇦\\n\\nІ підписуйтесь, хто ще цього не зробив:\\n@khreschatyk36']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request_data = {'texts': test_dataset['text'].sample(5).to_list()}\n",
    "request_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c28b8218-dd11-4a7e-acf9-7f426ee50c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[\"Микола\"],[],[\"Києві\"],[\"Україні\",\"Київ\"],[\"Київ\"]]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.post('http://localhost:8088/extract_locations', json=request_data).text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eb1266-e562-4d41-9f13-f3dcb7f47c00",
   "metadata": {},
   "source": [
    "As we can see, distilled model misses a lot of locations and mostly predicts obvious ones: \"Київ...\", \"Україна\", etc.\n",
    "\n",
    "This is probably happens because distilled model was trained on very small dataset and didn't see a lot of rare locations.\n",
    "\n",
    "To solve this, model should be trained on bigger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40611beb-b9a1-48a8-9050-b6ce5fb9f0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
