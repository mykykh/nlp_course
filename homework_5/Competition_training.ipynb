{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37c8c776-9e63-4d21-a5de-ad87ad1b1cdf"
   },
   "source": [
    "# Location detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12424ac6-6d53-4c9a-aab5-7e305e919081"
   },
   "source": [
    "The task is to detect location names from ukrainian and russian texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FASXO5Gu29Wz",
    "outputId": "f2cb1600-40d6-4df5-f7f6-e13d5ca4e52b"
   },
   "outputs": [],
   "source": [
    "!pip install datasets transformers evaluate seqeval pycld2 peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "du17Kgv64bgu",
    "outputId": "c65c118a-5057-4133-f91f-146427f4356b"
   },
   "outputs": [],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6c044eaf-bfd3-4ab4-9df4-68a4e00258a3"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "import string\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, get_linear_schedule_with_warmup\n",
    "from transformers import Trainer, pipeline\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftConfig, PeftModel\n",
    "import gc\n",
    "import pycld2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gg_S0SVdLZt7",
    "outputId": "ecbff105-d1e4-4ea9-91e7-164d9fcf238a"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download uk_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41f7ba99-72d7-4a52-bdbb-c6f64eaa5eef"
   },
   "outputs": [],
   "source": [
    "global_device = 'cpu'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    global_device = 'cuda'\n",
    "\n",
    "DATA_DIR = \"../data/location_detection/\"\n",
    "os.environ['WANDB_DISABLED']='true'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb8d6841-7077-4df1-88bb-1e6afcb40982"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8ea5c55-f8a0-480a-8c15-9f564c3b210f"
   },
   "outputs": [],
   "source": [
    "uk_dataset = pd.read_csv(DATA_DIR + \"uk_geo_dataset.csv\", converters={'loc_markers': eval})\n",
    "ru_dataset = pd.read_csv(DATA_DIR + \"ru_geo_dataset.csv\", converters={'loc_markers': eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e05f0663-fecb-4913-810e-37ef4f410a2c"
   },
   "outputs": [],
   "source": [
    "uk_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d588b0fe-c779-4415-99db-f3834e6c9e79"
   },
   "outputs": [],
   "source": [
    "ru_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10c0895d-44bb-4855-93bd-58d9c5b1bc61"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51e10731-5123-40a8-803b-bfde19dc4428"
   },
   "source": [
    "Using metric function from kaggle competition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dde1f19c-f394-436f-9ad5-eed0ac67deea"
   },
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    text = re.sub(r'[{re.escape(string.punctuation)}]', '', text)\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    text = re.sub(r'\\b\\w\\b\\s?]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.lower()\n",
    "\n",
    "def metric(y_true, y_pred):\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    tp, fp, fn, p = 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    for y_true_sample, y_pred_sample in zip(y_true, y_pred):\n",
    "        y_true_sample = set([process_text(s) for s in y_true_sample])\n",
    "        y_pred_sample = set([process_text(s) for s in y_pred_sample])\n",
    "\n",
    "        tp += len(y_true_sample & y_pred_sample)\n",
    "        fp += len(y_pred_sample - y_true_sample)\n",
    "        fn += len(y_true_sample - y_pred_sample)\n",
    "        p += len(y_true_sample)\n",
    "\n",
    "    if tp + fp == 0:\n",
    "        if p == 0:\n",
    "            precision = 1.0\n",
    "        else:\n",
    "            precision = 0.0\n",
    "    else:\n",
    "        precision = tp/(tp + fp)\n",
    "\n",
    "    if tp + fn == 0:\n",
    "        if p == 0:\n",
    "            recall = 1.0\n",
    "        else:\n",
    "            recall = 0.0\n",
    "    else:\n",
    "        recall = tp/(tp+fn)\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b52d2fc2-d3dd-4b8f-a29b-62986ab49bea"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38297ea8-7c2a-4402-9472-b7373b2a1b8a"
   },
   "source": [
    "To mark loaction using two tokens: begining of location (B-LOC) and inside of location (I-LOC).\n",
    "\n",
    "This is necesary, because with using only one LOC token two different locations and one location with two words in it would look the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c34839a0-6761-4728-bde1-d88aa8836b76"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('uk_core_news_sm', disable=['tagger', 'parser', 'ner', 'texcat'])\n",
    "\n",
    "def tokenize(texts, all_texts_loc_markers, tokenizer, batch_size=128, n_process=-1):\n",
    "    all_texts_tokenized = list(tokenizer.pipe(texts, batch_size=batch_size, n_process=n_process))\n",
    "\n",
    "    result_tokens = []\n",
    "    result_labels = []\n",
    "    for tokenized_text, loc_markers in zip(all_texts_tokenized, all_texts_loc_markers):\n",
    "        tokens = [token.text for token in tokenized_text]\n",
    "        labels = ['O'] * len(tokenized_text)\n",
    "\n",
    "        for idx, token in enumerate(tokenized_text):\n",
    "            for start, end in loc_markers:\n",
    "                if token.idx >= start and token.idx + len(token.text) <= end:\n",
    "                    if token.idx == start:\n",
    "                        # If token start pos == marker start,\n",
    "                        # then it is begining of new location name\n",
    "                        labels[idx] = 'B-LOC'\n",
    "                    else:\n",
    "                        # If token start pos > marker start,\n",
    "                        # then it is inside of location name\n",
    "                        labels[idx] = 'I-LOC'\n",
    "        result_tokens.append(tokens)\n",
    "        result_labels.append(labels)\n",
    "\n",
    "    return result_tokens, result_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ee3e2b83-3893-4111-9522-d78ce8ef71ec"
   },
   "outputs": [],
   "source": [
    "def process_dataset(dataset, result_path, add_col=None, n_splits=100):\n",
    "    try:\n",
    "        os.remove(result_path)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    for split in tqdm(np.array_split(dataset, n_splits), total=n_splits, bar_format='{l_bar}{bar:100}{r_bar}'):\n",
    "        tokens, labels = tokenize(split['text'].to_list(), split['loc_markers'].to_list(), nlp)\n",
    "        if add_col is not None:\n",
    "            result_df = pd.DataFrame({'tokens': tokens, 'labels': labels, add_col: split[add_col].to_list()})\n",
    "        else:\n",
    "            result_df = pd.DataFrame({'tokens': tokens, 'labels': labels})\n",
    "        if not os.path.isfile(result_path):\n",
    "            result_df.to_parquet(result_path, engine='fastparquet')\n",
    "        else:\n",
    "            result_df.to_parquet(result_path, engine='fastparquet', append=True)\n",
    "\n",
    "    # Save fastparquet as pyarrow\n",
    "    result_df = pd.read_parquet(result_path, engine='fastparquet')\n",
    "    result_df.to_parquet(result_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adevrarial validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_test_dataset = pd.read_csv(DATA_DIR + 'competition/test.csv', converters = {'locations': eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_uk_dataset = uk_dataset.sample(len(av_test_dataset))\n",
    "av_uk_dataset['is_test'] = 0\n",
    "av_test_dataset['is_test'] = 1\n",
    "av_dataset = pd.concat([av_uk_dataset, av_test_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_vectorizer = TfidfVectorizer().fit(av_dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    av_vectorizer.transform(av_dataset['text']),\n",
    "    av_dataset['is_test']\n",
    ")\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(X_train, y_train)\n",
    "av_pred = gbc.predict(X_test)\n",
    "roc_auc_score(y_test, av_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_dataset['is_pred_test'] = gbc.predict(av_vectorizer.transform(uk_dataset['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_dataset = uk_dataset[uk_dataset['is_pred_test'] == 1]\n",
    "len(uk_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f88c5292-7498-4a83-a363-d5c6ad771822"
   },
   "source": [
    "## Save datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "da99bfbd-d773-44dd-899f-25dc6a1e0f4f",
    "outputId": "1754e42a-8e67-497b-8726-25b1945f7320"
   },
   "outputs": [],
   "source": [
    "uk_train_dataset = uk_dataset[uk_dataset['is_valid'] == False].reset_index(drop=True)\n",
    "uk_holdout_dataset = uk_dataset[uk_dataset['is_valid'] == True].reset_index(drop=True)\n",
    "process_dataset(uk_train_dataset.sample(frac=0.01), DATA_DIR + 'uk_geo_dataset_processed_train_av.parquet', 'location_count', n_splits=10)\n",
    "process_dataset(uk_holdout_dataset.sample(frac=0.01), DATA_DIR + 'uk_geo_dataset_processed_holdout_av.parquet', n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7wRt5nqY7Uh9"
   },
   "outputs": [],
   "source": [
    "ru_train_dataset = ru_dataset[ru_dataset['doc_id'] <= 700000].reset_index(drop=True)\n",
    "ru_holdout_dataset = ru_dataset[ru_dataset['doc_id'] > 700000].reset_index(drop=True)\n",
    "process_dataset(ru_train_dataset.sample(frac=0.001), DATA_DIR + 'ru_geo_dataset_processed_train.parquet', 'doc_id')\n",
    "process_dataset(ru_holdout_dataset.sample(frac=0.001), DATA_DIR + 'ru_geo_dataset_processed_holdout.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a354ad52-0e74-457f-b961-1b20ba0bfe5b",
    "outputId": "3fce542f-a2b2-4922-d9d9-5a6d3998409a"
   },
   "outputs": [],
   "source": [
    "uk_train_processed_dataset = load_dataset(\n",
    "    'parquet',\n",
    "    data_files=DATA_DIR + 'uk_geo_dataset_processed_train_av.parquet',\n",
    "    split='train'\n",
    ")\n",
    "uk_holdout_processed_dataset = load_dataset(\n",
    "    'parquet',\n",
    "    data_files=DATA_DIR + 'uk_geo_dataset_processed_holdout_av.parquet',\n",
    "    split='train'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2K3A9wDCEiFU",
    "outputId": "685b24b2-53b3-4dc5-ee7d-52eaa9168681"
   },
   "outputs": [],
   "source": [
    "ru_train_processed_dataset = load_dataset(\n",
    "    'parquet',\n",
    "    data_files=DATA_DIR + 'ru_geo_dataset_processed_train.parquet',\n",
    "    split='train'\n",
    ")\n",
    "ru_holdout_processed_dataset = load_dataset(\n",
    "    'parquet',\n",
    "    data_files=DATA_DIR + 'ru_geo_dataset_processed_holdout.parquet',\n",
    "    split='train'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6f955dc-f864-46a7-ab50-e331a3cc7340"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "db3a5d2d-32e9-44f0-bcaa-b6e043e5c252",
    "outputId": "1a608240-8e4c-4d92-9d21-7aaf25f44840"
   },
   "outputs": [],
   "source": [
    "model_name = 'xlm-roberta-base'\n",
    "\n",
    "labels = ['S', 'O', 'B-LOC', 'I-LOC']\n",
    "label2id = {l: i for i, l in enumerate(labels)}\n",
    "id2label = {i: l for i, l in enumerate(labels)}\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3508361-3519-45ae-88b1-ef90a6515bc5"
   },
   "source": [
    "## Aligning labels with BERT tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d08216fe-dcd4-4021-b910-86baef47a7ab"
   },
   "outputs": [],
   "source": [
    "def align_labels_with_word_ids(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            # special tokens\n",
    "            current_word = word_id\n",
    "            new_labels.append(label2id[\"S\"])\n",
    "        elif word_id != current_word:\n",
    "            # start of new word\n",
    "            current_word = word_id\n",
    "            new_labels.append(label2id[labels[word_id]])\n",
    "        else:\n",
    "            # part of a word\n",
    "            label = labels[word_id]\n",
    "\n",
    "            if label == \"B-LOC\":\n",
    "                label = \"I-LOC\"\n",
    "\n",
    "            new_labels.append(label2id[label])\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def align_labels(examples):\n",
    "    bert_tokens = bert_tokenizer(examples['tokens'], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(examples['labels']):\n",
    "        word_ids = bert_tokens.word_ids(i)\n",
    "        new_labels.append(align_labels_with_word_ids(labels, word_ids))\n",
    "\n",
    "    bert_tokens['labels'] = new_labels\n",
    "    return bert_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6b28c06d-ad8b-4b85-bb74-9aa0c11fd5d6",
    "outputId": "a37b30d7-605c-4410-d818-269f9373ac88"
   },
   "outputs": [],
   "source": [
    "uk_train_processed_dataset = uk_train_processed_dataset.map(\n",
    "    align_labels,\n",
    "    batched=True\n",
    ")\n",
    "uk_holdout_processed_dataset = uk_holdout_processed_dataset.map(\n",
    "    align_labels,\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXO0crW2IcYw",
    "outputId": "70504804-b63c-469f-94a6-60f70926c4f6"
   },
   "outputs": [],
   "source": [
    "ru_train_processed_dataset = ru_train_processed_dataset.map(\n",
    "    align_labels,\n",
    "    batched=True\n",
    ")\n",
    "ru_holdout_processed_dataset = ru_holdout_processed_dataset.map(\n",
    "    align_labels,\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47013a35-9843-4c33-b412-4edb344f0f87"
   },
   "source": [
    "## Training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQBhIqV25oEJ",
    "outputId": "664bc71f-fb40-40ae-9875-2730e56adf52"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "  logits, labels = eval_preds\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "  true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "  prediction_label = [[id2label[p] for p, l in zip(prediction, label) if l != -100]\n",
    "                      for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "  all_metrics = metric.compute(predictions=prediction_label, references=true_labels)\n",
    "  return {\n",
    "      'precision': all_metrics['overall_precision'],\n",
    "      'recall': all_metrics['overall_recall'],\n",
    "      'f1': all_metrics['overall_f1']\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ac907cf-965c-4c48-b7ad-8bd9c4c1751d"
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "  model = AutoModelForTokenClassification.from_pretrained(\n",
    "      model_name,\n",
    "      label2id=label2id,\n",
    "      id2label=id2label\n",
    "  )\n",
    "  lora_config = LoraConfig(\n",
    "      task_type=TaskType.TOKEN_CLS,\n",
    "      inference_mode=False,\n",
    "      r=64,\n",
    "      lora_alpha=32,\n",
    "      lora_dropout=0.1\n",
    "  )\n",
    "  return get_peft_model(model, lora_config)\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRCrp8bwGUeV"
   },
   "outputs": [],
   "source": [
    "def get_opt_sched(model, dataset_len, batch_size):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, \n",
    "        T_0=1000\n",
    "    )\n",
    "\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9c076e39-6b91-4a2c-890e-edbf5513c637"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "db67c303-edc4-4d31-ab0d-d065f617604f"
   },
   "source": [
    "Training models for best_epoch_number on all train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7F9yPvaf4JX"
   },
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "def get_train_args(name):\n",
    "  return TrainingArguments(\n",
    "      name,\n",
    "      overwrite_output_dir=True,\n",
    "      per_device_train_batch_size=batch_size,\n",
    "      per_device_eval_batch_size=batch_size,\n",
    "      evaluation_strategy='steps',\n",
    "      save_strategy='steps',\n",
    "      logging_strategy='steps',\n",
    "      eval_steps=100,\n",
    "      logging_steps=100,\n",
    "      save_steps=100,\n",
    "      num_train_epochs=1,\n",
    "      fp16=True,\n",
    "      dataloader_pin_memory=False\n",
    "  )\n",
    "\n",
    "def train(model, train_dataset, test_dataset, args):\n",
    "  opt, sched = get_opt_sched(model, len(train_dataset), batch_size)\n",
    "  trainer = Trainer(\n",
    "      model=model,\n",
    "      args=args,\n",
    "      train_dataset=train_dataset,\n",
    "      eval_dataset=test_dataset,\n",
    "      data_collator=data_collator,\n",
    "      tokenizer=bert_tokenizer,\n",
    "      compute_metrics=compute_metrics,\n",
    "      optimizers=(opt, sched)\n",
    "  )\n",
    "  trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGnsftNUh4rP",
    "outputId": "2c6a4e7b-50e1-47f6-d0c5-6036d897c530"
   },
   "outputs": [],
   "source": [
    "uk_model = get_model()\n",
    "uk_train_args = get_train_args(DATA_DIR + 'models/uk-loc')\n",
    "train(uk_model, uk_train_processed_dataset, uk_holdout_processed_dataset, uk_train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del uk_model, uk_train_processed_dataset\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_model = get_model()\n",
    "ru_train_args = get_train_args(DATA_DIR + 'models/ru-loc')\n",
    "train(ru_model, ru_train_processed_dataset, ru_holdout_processed_dataset, ru_train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ru_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "affcca8a-86c1-4e6d-a695-b8958994cd7d"
   },
   "source": [
    "## Kaggle submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Orc709jBMjh2"
   },
   "outputs": [],
   "source": [
    "uk_checkpoint = DATA_DIR + 'models/uk-loc/checkpoint-2000'\n",
    "ru_checkpoint = DATA_DIR + 'models/ru-loc/checkpoint-1600'\n",
    "\n",
    "uk_inference = AutoModelForTokenClassification.from_pretrained(model_name, label2id=label2id, id2label=id2label)\n",
    "ru_inference = AutoModelForTokenClassification.from_pretrained(model_name, label2id=label2id, id2label=id2label)\n",
    "\n",
    "uk_inference = PeftModel.from_pretrained(uk_inference, uk_checkpoint).merge_and_unload()\n",
    "ru_inference = PeftModel.from_pretrained(ru_inference, ru_checkpoint).merge_and_unload()\n",
    "\n",
    "uk_classifier = pipeline(\n",
    "    'token-classification', model=uk_inference, tokenizer=bert_tokenizer, aggregation_strategy='simple'\n",
    ")\n",
    "ru_classifier = pipeline(\n",
    "    'token-classification', model=ru_inference, tokenizer=bert_tokenizer, aggregation_strategy='simple'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugAumWr254Pp"
   },
   "outputs": [],
   "source": [
    "def filter_text(text):\n",
    "  pattern = re.compile('['\n",
    "        '\\U0001F600-\\U0001F64F'\n",
    "        '\\U0001F300-\\U0001F5FF'\n",
    "        '\\U0001F680-\\U0001F6FF'\n",
    "        '\\U00010000-\\U00010FFF'\n",
    "        '\\U000024C2-\\U0001F251'\n",
    "        '\\u2600-\\u2B55'\n",
    "  ']+')\n",
    "  text = pattern.sub('', text)\n",
    "  text = re.sub(r'https?://\\S+', '', text)\n",
    "  text = re.sub(r'\\n', ' ', text)\n",
    "  text = re.sub(r' +', ' ', text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73aaecd1-4c68-4978-9582-940ef611fb1f"
   },
   "outputs": [],
   "source": [
    "competition_test = pd.read_csv(DATA_DIR + 'competition/test.csv', converters = {'locations': eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "139d2d7c-ad50-4432-8453-37e011b933ea",
    "outputId": "4eb8db64-b324-4ed9-e58c-cc0365a0f595"
   },
   "outputs": [],
   "source": [
    "competition_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zy4cBaYZDt_M",
    "outputId": "600f3608-8cec-4369-f687-189fcd714852"
   },
   "outputs": [],
   "source": [
    "competition_test['text'].head().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1MT_J7B5w8W",
    "outputId": "e92d87ba-1d13-4314-d025-3862a1bc5362"
   },
   "outputs": [],
   "source": [
    "competition_test['filtered_text'] = competition_test['text'].apply(filter_text)\n",
    "competition_test['filtered_text'].head().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MP4-gAjxGSot",
    "outputId": "5c0eb821-b289-4009-9453-8ed845f1c5cb"
   },
   "outputs": [],
   "source": [
    "def get_language_code(text):\n",
    "    lang = pycld2.detect(text)[2][0][1]\n",
    "    return 'ru' if lang == 'ru' else 'uk'\n",
    "\n",
    "competition_test['language'] = competition_test['text'].apply(get_language_code)\n",
    "set(competition_test['language'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4d331fd4-751c-488b-a96e-6deb46dbde3a",
    "outputId": "a27ba23f-f694-4b11-c7be-d587f8af4041"
   },
   "outputs": [],
   "source": [
    "competition_test.loc[competition_test['language'] == 'uk', 'locations'] = [\n",
    "    [p['word'] for p in s] for s in\n",
    "    uk_classifier(competition_test[competition_test['language'] == 'uk']['filtered_text'].to_list())\n",
    "]\n",
    "competition_test.loc[competition_test['language'] == 'ru', 'locations'] = [\n",
    "    [p['word'] for p in s] for s in\n",
    "    ru_classifier(competition_test[competition_test['language'] == 'ru']['filtered_text'].to_list())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "droped = []\n",
    "def post_process(locations):\n",
    "    ppl = [l for l in locations if (len(l) > 3 and any(c.isupper() for c in l)) or l == 'РФ' or l == 'США' or l == 'РБ']\n",
    "    droped.extend([l for l in locations if l not in ppl])\n",
    "    return ppl\n",
    "\n",
    "competition_test['locations'] = competition_test['locations'].apply(post_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "droped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "competition_test[['text', 'locations']].sample(10).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9241ae67-1b4d-45d9-a89f-c84dc4574eea",
    "outputId": "f18b8fb0-25e6-4d04-f9af-f415c1afb1c5"
   },
   "outputs": [],
   "source": [
    "example_with_many_locations = competition_test.iloc[\n",
    "    np.argsort(competition_test['locations'].apply(len)).iloc[-1]]\n",
    "\n",
    "print(f\"\"\"\n",
    "        Text: {example_with_many_locations['text']},\n",
    "        Pred locations: {example_with_many_locations['locations']}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ef4d88e-a076-49e7-acba-e5cf24e62518"
   },
   "outputs": [],
   "source": [
    "competition_test[['text_id', 'locations']].to_csv('/kaggle/working/roberta_base_lora_av.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3b356f0-0492-4596-bc95-fb5c6abc513f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3955219,
     "sourceId": 6884283,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3973548,
     "sourceId": 6920054,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30559,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
